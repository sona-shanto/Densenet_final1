{
  "cells": [
    {
      "metadata": {
        "id": "1XvcaAzr2rjY"
      },
      "cell_type": "markdown",
      "source": [
        "### Install and import necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "F6FuL-jLn0wc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "id": "y4ElXcLopnKO",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gc\n",
        "import re\n",
        "\n",
        "import cv2\n",
        "import math\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "import pandas as pd\n",
        "\n",
        "import tensorflow as tf\n",
        "from IPython.display import SVG\n",
        "import efficientnet.tfkeras as efn\n",
        "from keras.utils import plot_model\n",
        "import tensorflow.keras.layers as L\n",
        "from keras.utils import model_to_dot\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "from tensorflow.keras.applications import DenseNet121\n",
        "\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import matplotlib.cm as cm\n",
        "from sklearn import metrics\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "tqdm.pandas()\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "import plotly.figure_factory as ff\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "np.random.seed(0)\n",
        "tf.random.set_seed(0)\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DyZKLcDg2yRi"
      },
      "cell_type": "markdown",
      "source": [
        "### Load the data and define hyperparameters"
      ]
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "id": "mneU8D9bpnKS",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "EPOCHS = 20\n",
        "SAMPLE_LEN = 100\n",
        "IMAGE_PATH = \"/content/drive/MyDrive/plant-pathology-2020-fgvc7/images\"\n",
        "TEST_PATH = \"/content/drive/MyDrive/plant-pathology-2020-fgvc7/test.csv\"\n",
        "TRAIN_PATH = \"/content/drive/MyDrive/plant-pathology-2020-fgvc7/train.csv\"\n",
        "SUB_PATH = \"/content/drive/MyDrive/plant-pathology-2020-fgvc7/sample_submission.csv\"\n",
        "\n",
        "sub = pd.read_csv(SUB_PATH)\n",
        "test_data = pd.read_csv(TEST_PATH)\n",
        "train_data = pd.read_csv(TRAIN_PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lSXP0xubpnKW",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "train_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XISwCeBjpnKa",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "test_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def load_image(image_id):\n",
        "    file_path = os.path.join(IMAGE_PATH, image_id + \".jpg\")  # Use os.path.join for platform compatibility\n",
        "\n",
        "    # Check if the image file exists\n",
        "    if not os.path.exists(file_path):\n",
        "        print(f\"Warning: Image file not found: {file_path}\")\n",
        "        return None  # Or raise an exception if you prefer\n",
        "\n",
        "    image = cv2.imread(file_path)\n",
        "\n",
        "    # Check if image was loaded successfully\n",
        "    if image is None:\n",
        "        print(f\"Warning: Failed to load image: {file_path}\")\n",
        "        return None  # Or raise an exception\n",
        "\n",
        "    return cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "train_images = train_data[\"image_id\"][:SAMPLE_LEN].progress_apply(load_image)\n",
        "\n",
        "# Handle potential None values in train_images (e.g., remove them)\n",
        "train_images = train_images.dropna()"
      ],
      "metadata": {
        "id": "mMhWcjCZpTji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_image(image_id):\n",
        "    file_path = os.path.join(IMAGE_PATH, image_id + \".jpg\") # Use os.path.join to ensure correct path construction\n",
        "    image = cv2.imread(file_path)\n",
        "    if image is not None: # Check if image was loaded successfully\n",
        "        return cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    else:\n",
        "        print(f\"Warning: Failed to load image: {file_path}\")\n",
        "        return None # Return None if image loading failed to handle in the next step\n",
        "\n",
        "train_images = train_data[\"image_id\"][:SAMPLE_LEN].progress_apply(load_image)\n",
        "train_images = train_images.dropna() # Remove entries with None (failed image loading)"
      ],
      "metadata": {
        "id": "vwlWcxqlp2Qh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "O6DgMHJz293K"
      },
      "cell_type": "markdown",
      "source": [
        "### Load sample images"
      ]
    },
    {
      "metadata": {
        "id": "aI1QC_K-3HjL"
      },
      "cell_type": "markdown",
      "source": [
        "## Visualize one leaf <a id=\"1.2\"></a>"
      ]
    },
    {
      "metadata": {
        "id": "UY_1XspN3Afg"
      },
      "cell_type": "markdown",
      "source": [
        "### Sample image"
      ]
    },
    {
      "metadata": {
        "_kg_hide-input": true,
        "id": "LNXpowwwpnKg",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "fig = px.imshow(cv2.resize(train_images[0], (205, 136)))\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6iwEoQ90WnnA"
      },
      "cell_type": "markdown",
      "source": [
        "I have plotted the first image in the training data above (the RGB values can be seen by hovering over the image). The green parts of the image have very low blue values, but by contrast, the brown parts have high blue values. This suggests that green (healthy) parts of the image have low blue values, whereas unhealthy parts are more likely to have high blue values. **This might suggest that the blue channel may be the key to detecting diseases in plants.**"
      ]
    },
    {
      "metadata": {
        "id": "JGNq3QwJ3vs9"
      },
      "cell_type": "markdown",
      "source": [
        "## Channel distributions <a id=\"1.3\"></a>"
      ]
    },
    {
      "metadata": {
        "_kg_hide-input": true,
        "id": "SKlfmsiR3yVn",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "red_values = [np.mean(train_images[idx][:, :, 0]) for idx in range(len(train_images))]\n",
        "green_values = [np.mean(train_images[idx][:, :, 1]) for idx in range(len(train_images))]\n",
        "blue_values = [np.mean(train_images[idx][:, :, 2]) for idx in range(len(train_images))]\n",
        "values = [np.mean(train_images[idx]) for idx in range(len(train_images))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cluIike23zBw"
      },
      "cell_type": "markdown",
      "source": [
        "### All channel values"
      ]
    },
    {
      "metadata": {
        "_kg_hide-input": true,
        "id": "Eb32WiwY34At",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "fig = ff.create_distplot([values], group_labels=[\"Channels\"], colors=[\"purple\"])\n",
        "fig.update_layout(showlegend=False, template=\"simple_white\")\n",
        "fig.update_layout(title_text=\"Distribution of channel values\")\n",
        "fig.data[0].marker.line.color = 'rgb(0, 0, 0)'\n",
        "fig.data[0].marker.line.width = 0.5\n",
        "fig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Xr1kMIFtVAB_"
      },
      "cell_type": "markdown",
      "source": [
        "The channel values seem to have a roughly normal distribution centered around 105. The maximum channel activation is 255. This means that the average channel value is less than half the maximum value, which indicates that channels are minimally activated most of the time."
      ]
    },
    {
      "metadata": {
        "id": "utld_a6W37SB"
      },
      "cell_type": "markdown",
      "source": [
        "### Red channel values"
      ]
    },
    {
      "metadata": {
        "_kg_hide-input": true,
        "id": "jDIqxHgj3_zj",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "fig = ff.create_distplot([red_values], group_labels=[\"R\"], colors=[\"red\"])\n",
        "fig.update_layout(showlegend=False, template=\"simple_white\")\n",
        "fig.update_layout(title_text=\"Distribution of red channel values\")\n",
        "fig.data[0].marker.line.color = 'rgb(0, 0, 0)'\n",
        "fig.data[0].marker.line.width = 0.5\n",
        "fig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "r6HLx5fYVDIi"
      },
      "cell_type": "markdown",
      "source": [
        "The red channel values seem to roughly normal distribution, but with a slight rightward (positive skew). This indicates that the red channel tends to be more concentrated at lower values, at around 100. There is large variation in average red values across images."
      ]
    },
    {
      "metadata": {
        "id": "7Gr6JXgg4QAo"
      },
      "cell_type": "markdown",
      "source": [
        "### Green channel values"
      ]
    },
    {
      "metadata": {
        "_kg_hide-input": true,
        "id": "75DRD2Jy4Sp-",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "fig = ff.create_distplot([green_values], group_labels=[\"G\"], colors=[\"green\"])\n",
        "fig.update_layout(showlegend=False, template=\"simple_white\")\n",
        "fig.update_layout(title_text=\"Distribution of green channel values\")\n",
        "fig.data[0].marker.line.color = 'rgb(0, 0, 0)'\n",
        "fig.data[0].marker.line.width = 0.5\n",
        "fig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nkpWr3JTVTQe"
      },
      "cell_type": "markdown",
      "source": [
        "The green channel values have a more uniform distribution than the red channel values, with a smaller peak. The distribution also has a leftward skew (in contrast to red) and a larger mode of around 140. This indicates that green is more pronounced in these images than red, which makes sense, because these are images of leaves!"
      ]
    },
    {
      "metadata": {
        "id": "Dqs2zNPC4ZkG"
      },
      "cell_type": "markdown",
      "source": [
        "### Blue channel values"
      ]
    },
    {
      "metadata": {
        "_kg_hide-input": true,
        "id": "03Sp06S54bpy",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "fig = ff.create_distplot([blue_values], group_labels=[\"B\"], colors=[\"blue\"])\n",
        "fig.update_layout(showlegend=False, template=\"simple_white\")\n",
        "fig.update_layout(title_text=\"Distribution of blue channel values\")\n",
        "fig.data[0].marker.line.color = 'rgb(0, 0, 0)'\n",
        "fig.data[0].marker.line.width = 0.5\n",
        "fig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "L7dmewbgV4Cg"
      },
      "cell_type": "markdown",
      "source": [
        "The blue channel has the most uniform distribution out of the three color channels, with minimal skew (slight leftward skew). The blue channel shows great variation across images in the dataset."
      ]
    },
    {
      "metadata": {
        "id": "jjFXtVFM4l0w"
      },
      "cell_type": "markdown",
      "source": [
        "### All channel values (together)"
      ]
    },
    {
      "metadata": {
        "_kg_hide-input": true,
        "id": "iUJeF2Ae4oUl",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "fig = go.Figure()\n",
        "\n",
        "for idx, values in enumerate([red_values, green_values, blue_values]):\n",
        "    if idx == 0:\n",
        "        color = \"Red\"\n",
        "    if idx == 1:\n",
        "        color = \"Green\"\n",
        "    if idx == 2:\n",
        "        color = \"Blue\"\n",
        "    fig.add_trace(go.Box(x=[color]*len(values), y=values, name=color, marker=dict(color=color.lower())))\n",
        "\n",
        "fig.update_layout(yaxis_title=\"Mean value\", xaxis_title=\"Color channel\",\n",
        "                  title=\"Mean value vs. Color channel\", template=\"plotly_white\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_kg_hide-input": true,
        "id": "2x8aqxw04o8u",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "fig = ff.create_distplot([red_values, green_values, blue_values],\n",
        "                         group_labels=[\"R\", \"G\", \"B\"],\n",
        "                         colors=[\"red\", \"green\", \"blue\"])\n",
        "fig.update_layout(title_text=\"Distribution of red channel values\", template=\"simple_white\")\n",
        "fig.data[0].marker.line.color = 'rgb(0, 0, 0)'\n",
        "fig.data[0].marker.line.width = 0.5\n",
        "fig.data[1].marker.line.color = 'rgb(0, 0, 0)'\n",
        "fig.data[1].marker.line.width = 0.5\n",
        "fig.data[2].marker.line.color = 'rgb(0, 0, 0)'\n",
        "fig.data[2].marker.line.width = 0.5\n",
        "fig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7tUmWvzYWMSs"
      },
      "cell_type": "markdown",
      "source": [
        "From the above plots, we can clearly see which colors are more common and which ones less common in the leaf images. Green is the most pronounced color, followed by red and blue respectively. The distributions, when plotted together, appear to have a similar shape, but shifted horizontally."
      ]
    },
    {
      "metadata": {
        "id": "Ee2qutMw3FTP"
      },
      "cell_type": "markdown",
      "source": [
        "## Visualize sample leaves <a id=\"1.4\"></a>\n",
        "\n",
        "Now, I will visualize sample leaves beloning to different categories in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_leaves(cond=[0, 0, 0, 0], cond_cols=[\"healthy\"], is_cond=True):\n",
        "    if not is_cond:\n",
        "        cols, rows = 3, min([3, len(train_images)//3])\n",
        "        fig, ax = plt.subplots(nrows=rows, ncols=cols, figsize=(30, rows*20/3))\n",
        "        for col in range(cols):\n",
        "            for row in range(rows):\n",
        "                ax[row, col].imshow(train_images.loc[train_images.index[-row*3-col-1]])\n",
        "        return None\n",
        "\n",
        "    cond_0 = \"healthy == {}\".format(cond[0])\n",
        "    cond_1 = \"scab == {}\".format(cond[1])\n",
        "    cond_2 = \"rust == {}\".format(cond[2])\n",
        "    cond_3 = \"multiple_diseases == {}\".format(cond[3])\n",
        "\n",
        "    cond_list = []\n",
        "    for col in cond_cols:\n",
        "        if col == \"healthy\":\n",
        "            cond_list.append(cond_0)\n",
        "        if col == \"scab\":\n",
        "            cond_list.append(cond_1)\n",
        "        if col == \"rust\":\n",
        "            cond_list.append(cond_2)\n",
        "        if col == \"multiple_diseases\":\n",
        "            cond_list.append(cond_3)\n",
        "\n",
        "    # The error was happening because data was selecting rows with indices up to 100.\n",
        "    # However, train_images only has entries up to 99 since it was generated in cell 18\n",
        "    # using the expression: train_data[\"image_id\"][:SAMPLE_LEN].progress_apply(load_image)\n",
        "    # Therefore, limiting data to entries below 100 solves the problem.\n",
        "    data = train_data.loc[:SAMPLE_LEN-1] # Limit data to contain indices between 0 and 99.\n",
        "    for cond in cond_list:\n",
        "        data = data.query(cond)\n",
        "\n",
        "    images = train_images.loc[list(data.index)]\n",
        "    cols, rows = 3, min([3, len(images)//3])\n",
        "\n",
        "    fig, ax = plt.subplots(nrows=rows, ncols=cols, figsize=(30, rows*20/3))\n",
        "    for col in range(cols):\n",
        "        for row in range(rows):\n",
        "            ax[row, col].imshow(images.loc[images.index[row*3+col]])\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "VzFAQNNNqbC9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6YBTzqs53VHW"
      },
      "cell_type": "markdown",
      "source": [
        "### Healthy leaves"
      ]
    },
    {
      "metadata": {
        "_kg_hide-input": true,
        "id": "_Gw1CH7bpnLc",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "visualize_leaves(cond=[1, 0, 0, 0], cond_cols=[\"healthy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "v7T9CAFoXt9A"
      },
      "cell_type": "markdown",
      "source": [
        "In the above images, we can see that the healthy leaves are completely green, do not have any brown/yellow spots or scars. Healthy leaves do not have scab or rust."
      ]
    },
    {
      "metadata": {
        "id": "lfWbbGfD5ACD"
      },
      "cell_type": "markdown",
      "source": [
        "### Leaves with scab"
      ]
    },
    {
      "metadata": {
        "_kg_hide-input": true,
        "id": "cWTEw_JZpnLj",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "visualize_leaves(cond=[0, 1, 0, 0], cond_cols=[\"scab\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qHR_5GsKYQWz"
      },
      "cell_type": "markdown",
      "source": [
        "In the above images, we can see that leaves with \"scab\" have large brown marks and stains across the leaf. Scab is defined as \"any of various plant diseases caused by fungi or bacteria and resulting in crustlike spots on fruit, leaves, or roots. The spots caused by such a disease\". The brown marks across the leaf are a sign of these bacterial/fungal infections. Once diagnosed, scab can be treated using chemical or non-chemical methods."
      ]
    },
    {
      "metadata": {
        "id": "1doTwr2S5Dxt"
      },
      "cell_type": "markdown",
      "source": [
        "### Leaves with rust"
      ]
    },
    {
      "metadata": {
        "_kg_hide-input": true,
        "id": "Xwn6GVaupnLl",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "visualize_leaves(cond=[0, 0, 1, 0], cond_cols=[\"rust\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l_0g20pmZk17"
      },
      "cell_type": "markdown",
      "source": [
        "In the above images, we can see that leaves with \"rust\" have several brownish-yellow spots across the leaf. Rust is defined as \"a disease, especially of cereals and other grasses, characterized by rust-colored pustules of spores on the affected leaf blades and sheaths and caused by any of several rust fungi\". The yellow spots are a sign of infection by a special type of fungi called \"rust fungi\". Rust can also be treated with several chemical and non-chemical methods once diagnosed."
      ]
    },
    {
      "metadata": {
        "id": "mKALD2vt5G78"
      },
      "cell_type": "markdown",
      "source": [
        "### Leaves with multiple diseases"
      ]
    },
    {
      "metadata": {
        "_kg_hide-input": true,
        "id": "YQm_lj9TpnLo",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "visualize_leaves(cond=[0, 0, 0, 1], cond_cols=[\"multiple_diseases\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qyG-NpVDahDN"
      },
      "cell_type": "markdown",
      "source": [
        "In the above images, we can see that the leaves show symptoms for several diseases, including brown marks and yellow spots. These plants have more than one of the above-described diseases."
      ]
    },
    {
      "metadata": {
        "id": "yTFIf7dZ5MNe"
      },
      "cell_type": "markdown",
      "source": [
        "## Visualize targets <a id=\"1.5\"></a>\n",
        "\n",
        "Now, I will visualize the labels and target data. **In all the below plots, blue represents the \"desired\" or \"healthy\" condition, and red represents the \"undesired\" or \"unhealthy\" condition.**"
      ]
    },
    {
      "metadata": {
        "id": "rTU7Plrb5R2v"
      },
      "cell_type": "markdown",
      "source": [
        "### All labels together (parallel plot)"
      ]
    },
    {
      "metadata": {
        "_kg_hide-input": true,
        "id": "vRDHR4G_pnLs",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "fig = px.parallel_categories(train_data[[\"healthy\", \"scab\", \"rust\", \"multiple_diseases\"]], color=\"healthy\", color_continuous_scale=\"sunset\",\\\n",
        "                             title=\"Parallel categories plot of targets\")\n",
        "fig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "POYx63MXcfcQ"
      },
      "cell_type": "markdown",
      "source": [
        "In the above plot, we can see the relationship between all four categories. As expected, it is impossible for a healthy leaf (<code>healthy == 1</code>) to have scab, rust, or multiple diseases. Also, every unhealthy leaf has one of either scab, rust, or multiple diseases. The frequency of each combination can be seen by hovering over the plot."
      ]
    },
    {
      "metadata": {
        "id": "nibrWw7bngaf"
      },
      "cell_type": "markdown",
      "source": [
        "### Pie chart"
      ]
    },
    {
      "metadata": {
        "_kg_hide-input": true,
        "trusted": true,
        "id": "LpYKleM8ngaf"
      },
      "cell_type": "code",
      "source": [
        "fig = go.Figure([go.Pie(labels=train_data.columns[1:],\n",
        "           values=train_data.iloc[:, 1:].sum().values)])\n",
        "fig.update_layout(title_text=\"Pie chart of targets\", template=\"simple_white\")\n",
        "fig.data[0].marker.line.color = 'rgb(0, 0, 0)'\n",
        "fig.data[0].marker.line.width = 0.5\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gPaCYZf_ngag"
      },
      "cell_type": "markdown",
      "source": [
        "In the pie chart above, we can see that most leaves in the dataset are unhealthy (71.7%). Only 5% of plants have multiple diseases, and \"rust\" and \"scab\" occupy approximately one-third of the pie each."
      ]
    },
    {
      "metadata": {
        "id": "kJNU54Yy5WST"
      },
      "cell_type": "markdown",
      "source": [
        "### Healthy distribution"
      ]
    },
    {
      "metadata": {
        "_kg_hide-input": true,
        "id": "lriYl3yXpnLv",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "train_data[\"Healthy\"] = train_data[\"healthy\"].apply(bool).apply(str)\n",
        "fig = px.histogram(train_data, x=\"Healthy\", title=\"Healthy distribution\", color=\"Healthy\",\\\n",
        "            color_discrete_map={\n",
        "                \"True\": px.colors.qualitative.Plotly[0],\n",
        "                \"False\": px.colors.qualitative.Plotly[1]})\n",
        "fig.update_layout(template=\"simple_white\")\n",
        "fig.data[0].marker.line.color = 'rgb(0, 0, 0)'\n",
        "fig.data[0].marker.line.width = 0.5\n",
        "fig.data[1].marker.line.color = 'rgb(0, 0, 0)'\n",
        "fig.data[1].marker.line.width = 0.5\n",
        "fig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qewm9cFSes3y"
      },
      "cell_type": "markdown",
      "source": [
        "We can see that there are more unhealthy (<code>healthy == 0</code>) plants than healthy (<code>healthy == 1</code>) ones. There are 1305 (72%) unhealthy plants and 516 (28%) healthy plants."
      ]
    },
    {
      "metadata": {
        "id": "wDb-wmeA5ZMa"
      },
      "cell_type": "markdown",
      "source": [
        "### Scab distribution"
      ]
    },
    {
      "metadata": {
        "_kg_hide-input": true,
        "id": "AL3UmfaZpnLy",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "train_data[\"Scab\"] = train_data[\"scab\"].apply(bool).apply(str)\n",
        "fig = px.histogram(train_data, x=\"Scab\", color=\"Scab\", title=\"Scab distribution\",\\\n",
        "            color_discrete_map={\n",
        "                \"True\": px.colors.qualitative.Plotly[1],\n",
        "                \"False\": px.colors.qualitative.Plotly[0]})\n",
        "fig.update_layout(template=\"simple_white\")\n",
        "fig.data[0].marker.line.color = 'rgb(0, 0, 0)'\n",
        "fig.data[0].marker.line.width = 0.5\n",
        "fig.data[1].marker.line.color = 'rgb(0, 0, 0)'\n",
        "fig.data[1].marker.line.width = 0.5\n",
        "fig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q3s0XzhwfOc-"
      },
      "cell_type": "markdown",
      "source": [
        "We can see that there are more plants without scab (<code>scab == 0</code>) than those with scab (<code>scab == 1</code>). There are 592 (33%) unhealthy plants and 1229 (67%) healthy plants."
      ]
    },
    {
      "metadata": {
        "id": "fJ0g_MRy5bQb"
      },
      "cell_type": "markdown",
      "source": [
        "### Rust distribution"
      ]
    },
    {
      "metadata": {
        "_kg_hide-input": true,
        "id": "EamjMT0JpnL0",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "train_data[\"Rust\"] = train_data[\"rust\"].apply(bool).apply(str)\n",
        "fig = px.histogram(train_data, x=\"Rust\", color=\"Rust\", title=\"Rust distribution\",\\\n",
        "            color_discrete_map={\n",
        "                \"True\": px.colors.qualitative.Plotly[1],\n",
        "                \"False\": px.colors.qualitative.Plotly[0]})\n",
        "fig.update_layout(template=\"simple_white\")\n",
        "fig.data[0].marker.line.color = 'rgb(0, 0, 0)'\n",
        "fig.data[0].marker.line.width = 0.5\n",
        "fig.data[1].marker.line.color = 'rgb(0, 0, 0)'\n",
        "fig.data[1].marker.line.width = 0.5\n",
        "fig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7EWabO690nSX"
      },
      "cell_type": "markdown",
      "source": [
        "We can see that there are more plants without rust (<code>rust == 0</code>) than those with rust (<code>rust == 1</code>). There are 622 (34%) unhealthy plants and 1199 (66%) healthy plants. We can see that the \"unhealthy\" percentage is very similar for both rust and scab."
      ]
    },
    {
      "metadata": {
        "id": "S0t3SEpB5dcS"
      },
      "cell_type": "markdown",
      "source": [
        "### Multiple diseases distribution"
      ]
    },
    {
      "metadata": {
        "_kg_hide-input": true,
        "id": "9gYSWOl0pnL3",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "train_data[\"Multiple diseases\"] = train_data[\"multiple_diseases\"].apply(bool).apply(str)\n",
        "fig = px.histogram(train_data, x=\"Multiple diseases\", color=\"Multiple diseases\", title=\"Multiple diseases distribution\",\\\n",
        "            color_discrete_map={\n",
        "                \"True\": px.colors.qualitative.Plotly[1],\n",
        "                \"False\": px.colors.qualitative.Plotly[0]})\n",
        "fig.update_layout(template=\"simple_white\")\n",
        "fig.data[0].marker.line.color = 'rgb(0, 0, 0)'\n",
        "fig.data[0].marker.line.width = 0.5\n",
        "fig.data[1].marker.line.color = 'rgb(0, 0, 0)'\n",
        "fig.data[1].marker.line.width = 0.5\n",
        "fig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GDgVIT2p2YrZ"
      },
      "cell_type": "markdown",
      "source": [
        "We can see that very few leaves have multiple diseases (it is a very occurance). There are 91 (5%) unhealthy plants and 1730 (95%) healthy plants."
      ]
    },
    {
      "metadata": {
        "id": "aiPKQDkh5hF5"
      },
      "cell_type": "markdown",
      "source": [
        "# Image processing and augmentation <a id=\"2\"></a>"
      ]
    },
    {
      "metadata": {
        "id": "jC9wp-8N5kU4"
      },
      "cell_type": "markdown",
      "source": [
        "## Canny edge detection <a id=\"2.1\"></a>\n",
        "\n",
        "Canny is a popular edge detection algorithm, and as the name suggests, it detects the edges of objects present in an image. It was developed by John F. Canny in 1986. The algorithm involves several steps.\n",
        "\n",
        "1. **Noise reduction:** Since edge detection is susceptible to noise in an image, we remove the noise in the image using a 5x5 Gaussian filter.\n",
        "\n",
        "\n",
        "2. **Finding Intensity Gradient of the Image**: The smoothened image is then filtered with a Sobel kernel in both horizontal and vertical directions to get the first derivative in the horizontal (*G<sub>x</sub>*) and vertical (*G<sub>y</sub>*) directions. From these two images, one can find the edge gradient and direction for each pixel:\n",
        "\n",
        "<center><img src=\"https://i.imgur.com/ntyjTep.png\" width=\"300px\"></center>\n",
        "<center><img src=\"https://i.imgur.com/75qDjv6.png\" width=\"260px\"></center>\n",
        "\n",
        "<br>\n",
        "\n",
        "3. **Rounding:** The gradient is always perpendicular to edges. So, it is rounded to one of the four angles representing vertical, horizontal and two diagonal directions.\n",
        "\n",
        "4. **Non-maximum suppression:** After getting the gradient magnitude and direction, a full scan of the image is done to remove any unwanted pixels which may not constitute the edge. For this, we check every pixel for being a local maximum in its neighborhood in the direction of the gradient.\n",
        "\n",
        "5. **Hysteresis Thresholding:** This stage decides which parts are edges and which are not. For this, we need two threshold values, *minVal* and *maxVal*. Any edges with intensity gradient greater than *maxVal* are considered edges and those lesser than *minVal* are considered non-edges, and discarded. Those who lie between these two thresholds are classified edges or non-edges based on their neighborhood. If they are near “sure-edge” pixels, they are considered edges, and otherwise, they are discarded.\n",
        "\n",
        "The result of these five steps is a two-dimensional binary map (0 or 255) indicating the location of edges on the image. Canny edge is demonstrated below with a few leaf images:"
      ]
    },
    {
      "metadata": {
        "id": "s6TVlyMmngas"
      },
      "cell_type": "markdown",
      "source": [
        "\n"
      ]
    },
    {
      "metadata": {
        "_kg_hide-input": true,
        "id": "uU_iqYaCpnL7",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "def edge_and_cut(img):\n",
        "    emb_img = img.copy()\n",
        "    edges = cv2.Canny(img, 100, 200)\n",
        "    edge_coors = []\n",
        "    for i in range(edges.shape[0]):\n",
        "        for j in range(edges.shape[1]):\n",
        "            if edges[i][j] != 0:\n",
        "                edge_coors.append((i, j))\n",
        "\n",
        "    row_min = edge_coors[np.argsort([coor[0] for coor in edge_coors])[0]][0]\n",
        "    row_max = edge_coors[np.argsort([coor[0] for coor in edge_coors])[-1]][0]\n",
        "    col_min = edge_coors[np.argsort([coor[1] for coor in edge_coors])[0]][1]\n",
        "    col_max = edge_coors[np.argsort([coor[1] for coor in edge_coors])[-1]][1]\n",
        "    new_img = img[row_min:row_max, col_min:col_max]\n",
        "\n",
        "    emb_img[row_min-10:row_min+10, col_min:col_max] = [255, 0, 0]\n",
        "    emb_img[row_max-10:row_max+10, col_min:col_max] = [255, 0, 0]\n",
        "    emb_img[row_min:row_max, col_min-10:col_min+10] = [255, 0, 0]\n",
        "    emb_img[row_min:row_max, col_max-10:col_max+10] = [255, 0, 0]\n",
        "\n",
        "    fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(30, 20))\n",
        "    ax[0].imshow(img, cmap='gray')\n",
        "    ax[0].set_title('Original Image', fontsize=24)\n",
        "    ax[1].imshow(edges, cmap='gray')\n",
        "    ax[1].set_title('Canny Edges', fontsize=24)\n",
        "    ax[2].imshow(emb_img, cmap='gray')\n",
        "    ax[2].set_title('Bounding Box', fontsize=24)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_kg_hide-input": true,
        "id": "bYJ0t9kppnL9",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "edge_and_cut(train_images[3])\n",
        "edge_and_cut(train_images[4])\n",
        "edge_and_cut(train_images[5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q6Da0051ngau"
      },
      "cell_type": "markdown",
      "source": [
        "The second column of images above contains the Canny edges and the third column contains cropped images. I have taken the Canny edges and used it to predict a bounding box in which the actual leaf is contained. The most extreme edges at the four corners of the image are the vertices of the bounding box. This red box is likely to contain most of if not all of the leaf. These edges and bounding boxes can be used to build more accurate models."
      ]
    },
    {
      "metadata": {
        "id": "Wh0WHTCC5sL_"
      },
      "cell_type": "markdown",
      "source": [
        "## Flipping <a id=\"2.2\"></a>\n",
        "\n",
        "Flipping is a simple transformation that involves index-switching on the image channels. In vertical flipping, the order of rows is exchanged, whereas in vertical flipping, the order of rows is exchanged. Let us assume that *A<sub>ijk</sub>* (of size *(m, n, 3)*) is the image we want to flip. Horizontal and vertical flipping can be represented by the transformations below:\n",
        "\n",
        "<center><img src=\"https://i.imgur.com/B9y5apl.png\" width=\"135px\"></center>\n",
        "<center><img src=\"https://i.imgur.com/eQ1dyvN.png\" width=\"305px\"></center>\n",
        "<center><img src=\"https://i.imgur.com/i30LQgq.png\" width=\"305px\"></center>\n",
        "<br>\n",
        "\n",
        "We can see that the order of columns is exchanged in horizontal flipping. While the *i* and *k* indices remain the same, the *j* index reverses. Whereas, in vertical flipping, the order of rows is exchanged in horizontal flipping. While the *j* and *k* indices remain the same, the *i* index reverses.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "_kg_hide-input": true,
        "id": "XPZwEZAepnMA",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "def invert(img):\n",
        "    fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(30, 20))\n",
        "    ax[0].imshow(img)\n",
        "    ax[0].set_title('Original Image', fontsize=24)\n",
        "    ax[1].imshow(cv2.flip(img, 0))\n",
        "    ax[1].set_title('Vertical Flip', fontsize=24)\n",
        "    ax[2].imshow(cv2.flip(img, 1))\n",
        "    ax[2].set_title('Horizontal Flip', fontsize=24)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_kg_hide-input": true,
        "id": "gaiLRUzopnMD",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "invert(train_images[3])\n",
        "invert(train_images[4])\n",
        "invert(train_images[5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4C1BC9sHngax"
      },
      "cell_type": "markdown",
      "source": [
        "We can see that the images are simply flipped. All major features in the image remain the same, but to a computer algorithm, the flipped images look completely different. These transformations can be used for data augmentation, making models more robust and accurate."
      ]
    },
    {
      "metadata": {
        "id": "PqS2I93A5u_R"
      },
      "cell_type": "markdown",
      "source": [
        "## Convolution <a id=\"2.3\"></a>\n",
        "\n",
        "Convolution is a rather simple algorithm which involves a kernel (a 2D matrix) which moves over the entire image, calculating dot products with each window along the way. The GIF below demonstrates convolution in action.\n",
        "\n",
        "<center><img src=\"https://i.imgur.com/wYUaqR3.gif\" width=\"450px\"></center>\n",
        "\n",
        "The above process can be summarized with an equation, where *f* is the image and *h* is the kernel. The dimensions of *f* are *(m, n)* and the kernel is a square matrix with dimensions smaller than *f*:\n",
        "\n",
        "<center><img src=\"https://i.imgur.com/9scTOGv.png\" width=\"350px\"></center>\n",
        "<br>\n",
        "\n",
        "In the above equation, the kernel *h* is moving across the length and breadth of the image. The dot product of *h* with a sub-matrix or window of matrix *f* is taken at each step, hence the double summation (rows and columns). Below I demonstrate the effect of convolution on leaf images."
      ]
    },
    {
      "metadata": {
        "_kg_hide-input": true,
        "id": "aa81abmWpnMG",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "def conv(img):\n",
        "    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(20, 20))\n",
        "    kernel = np.ones((7, 7), np.float32)/25\n",
        "    conv = cv2.filter2D(img, -1, kernel)\n",
        "    ax[0].imshow(img)\n",
        "    ax[0].set_title('Original Image', fontsize=24)\n",
        "    ax[1].imshow(conv)\n",
        "    ax[1].set_title('Convolved Image', fontsize=24)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_kg_hide-input": true,
        "id": "2rqxzygspnMJ",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "conv(train_images[3])\n",
        "conv(train_images[4])\n",
        "conv(train_images[5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8eKcYUNwnga4"
      },
      "cell_type": "markdown",
      "source": [
        "The convolution operator seems to have an apparent \"sunshine\" effect of the images. This may also serve the purpose of augmenting the data, thus helping to build more robust and accurate models."
      ]
    },
    {
      "metadata": {
        "id": "xmmHZzUq5xu8"
      },
      "cell_type": "markdown",
      "source": [
        "## Blurring <a id=\"2.4\"></a>\n",
        "\n",
        "Blurring is simply the addition of noise to the image, resulting in a less-clear image. The noise can be sampled from any distribution of choice, as long as the main content in the image does not become invisible. Only the minor details get obfuscated due to blurring. The blurring transformation can be represented using the equation below.\n",
        "\n",
        "<center><img src=\"https://i.imgur.com/zVM8HCU.png\" width=\"220px\"></center>\n",
        "<br>\n",
        "\n",
        "The example uses a Gaussian distribution with mean 0 and variance 0.1. Below I demonstrate the effect of blurring on a few leaf images:"
      ]
    },
    {
      "metadata": {
        "_kg_hide-input": true,
        "id": "OcXTa-cxpnMM",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "def blur(img):\n",
        "    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(20, 20))\n",
        "    ax[0].imshow(img)\n",
        "    ax[0].set_title('Original Image', fontsize=24)\n",
        "    ax[1].imshow(cv2.blur(img, (100, 100)))\n",
        "    ax[1].set_title('Blurred Image', fontsize=24)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_kg_hide-input": true,
        "id": "aH5qRDdupnMP",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "blur(train_images[3])\n",
        "blur(train_images[4])\n",
        "blur(train_images[5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Oj4ALPZ4nga8"
      },
      "cell_type": "markdown",
      "source": [
        "The transformation clearly blurs the image by removing detailed, low-level features, while retaining the major, high-level features. This is once again a great way to augment images and train more robust models."
      ]
    },
    {
      "metadata": {
        "id": "wyfDeAJo6lGL"
      },
      "cell_type": "markdown",
      "source": [
        "# Modeling <a id=\"3\"></a>"
      ]
    },
    {
      "metadata": {
        "id": "J5f4-lkS7lKs"
      },
      "cell_type": "markdown",
      "source": [
        "## Preparing the ground <a id=\"3.1\"></a>\n",
        "\n",
        "Before we move on to building the models, I will explain the major building blocks in pretrained CV models. Every major ImageNet model has a different architecture, but each one has the common building blocks: **Conv2D, MaxPool, ReLU**. I have already explained the mechanism behind convolution in the previous section, so I will now explain MaxPool and ReLU.\n",
        "\n",
        "### MaxPool\n",
        "\n",
        "Max pooling is very similar to convolution, except it involves finding the maximum value in a window instead of finding the dot product of the window with a kernel. Max pooling does not require a kernel and it is very useful in reducing the dimensionality of convolutional feature maps in CNNs. The image below demonstrates the working of MaxPool:\n",
        "\n",
        "\n",
        "<center><img src=\"https://i.imgur.com/rBNMsfi.png\" width=\"400px\"></center>\n",
        "<br></br>\n",
        "\n",
        "The above example demonstrates max pooling with a window size of *(2, 2)*. This process can be represented with the equation below:\n",
        "<br></br>\n",
        ".\n",
        "\n",
        "<center><img src=\"https://i.imgur.com/FRyMNhI.png\" width=\"650px\"></center>\n",
        "<br></br>\n",
        "\n",
        "In the above equation, the window moves across the image and the maximum value in each winow is calculated. Once again, this process is very important in reducing the complexity of CNNs while retaining features."
      ]
    },
    {
      "metadata": {
        "id": "Ky2RVsSRnga9"
      },
      "cell_type": "markdown",
      "source": [
        "### ReLU\n",
        "\n",
        "ReLU is an activation function commonly used in neural network architectures. *ReLU(x)* returns 0 for *x < 0* and *x* otherwise. This function helps introducenon-linearity in the neural network, thus increasing its capacity ot model the image data. The graph and equation of *ReLU* are:\n",
        "\n",
        "<center><img src=\"https://i.imgur.com/eiRVQBh.png\" width=\"400px\"></center>\n",
        "\n",
        "<center><img src=\"https://i.imgur.com/0mBFAH0.png\" width=\"400px\"></center>\n",
        "<br></br>\n",
        "\n",
        "As mentioned earlier, this function is non-linear and helps increase the modeling capacity of the CNN models. Now since we understand the basic building blocks of pretrained images models, let us finetune some pretained ImageNet models on TPU and visualize the results!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install cloud-tpu-client\n",
        "import tensorflow as tf\n",
        "try:\n",
        "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
        "  print('Running on TPU ', tpu.master()) # Changed to tpu.master() to get TPU address\n",
        "except ValueError:\n",
        "  raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')"
      ],
      "metadata": {
        "id": "gIN4PyxxrTFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zOfbl73V6t3p"
      },
      "cell_type": "markdown",
      "source": [
        "### Setup TPU Config"
      ]
    },
    {
      "metadata": {
        "id": "2ZC6VPQHpnMR",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "AUTO = tf.data.experimental.AUTOTUNE\n",
        "tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "\n",
        "tf.config.experimental_connect_to_cluster(tpu)\n",
        "tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "\n",
        "BATCH_SIZE = 16 * strategy.num_replicas_in_sync\n",
        "GCS_DS_PATH = '/content/drive/MyDrive/plant-pathology-2020-fgvc7'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SuAHc2hu6-Nu"
      },
      "cell_type": "markdown",
      "source": [
        "### Load labels and paths"
      ]
    },
    {
      "metadata": {
        "id": "9BALmDtRpnMU",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "def format_path(st):\n",
        "    return GCS_DS_PATH + '/images/' + st + '.jpg'\n",
        "\n",
        "test_paths = test_data.image_id.apply(format_path).values\n",
        "train_paths = train_data.image_id.apply(format_path).values\n",
        "\n",
        "train_labels = np.float32(train_data.loc[:, 'healthy':'scab'].values)\n",
        "train_paths, valid_paths, train_labels, valid_labels =\\\n",
        "train_test_split(train_paths, train_labels, test_size=0.15, random_state=2020)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T84Nnc1jpnMW",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "def decode_image(filename, label=None, image_size=(512, 512)):\n",
        "    bits = tf.io.read_file(filename)\n",
        "    image = tf.image.decode_jpeg(bits, channels=3)\n",
        "    image = tf.cast(image, tf.float32) / 255.0\n",
        "    image = tf.image.resize(image, image_size)\n",
        "\n",
        "    if label is None:\n",
        "        return image\n",
        "    else:\n",
        "        return image, label\n",
        "\n",
        "def data_augment(image, label=None):\n",
        "    image = tf.image.random_flip_left_right(image)\n",
        "    image = tf.image.random_flip_up_down(image)\n",
        "\n",
        "    if label is None:\n",
        "        return image\n",
        "    else:\n",
        "        return image, label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tonEhhQ77Knh"
      },
      "cell_type": "markdown",
      "source": [
        "### Create Dataset objects"
      ]
    },
    {
      "metadata": {
        "id": "5rkIRCnupnMZ",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "train_dataset = (\n",
        "    tf.data.Dataset\n",
        "    .from_tensor_slices((train_paths, train_labels))\n",
        "    .map(decode_image, num_parallel_calls=AUTO)\n",
        "    .map(data_augment, num_parallel_calls=AUTO)\n",
        "    .repeat()\n",
        "    .shuffle(512)\n",
        "    .batch(BATCH_SIZE)\n",
        "    .prefetch(AUTO)\n",
        ")\n",
        "\n",
        "valid_dataset = (\n",
        "    tf.data.Dataset\n",
        "    .from_tensor_slices((valid_paths, valid_labels))\n",
        "    .map(decode_image, num_parallel_calls=AUTO)\n",
        "    .batch(BATCH_SIZE)\n",
        "    .cache()\n",
        "    .prefetch(AUTO)\n",
        ")\n",
        "\n",
        "test_dataset = (\n",
        "    tf.data.Dataset\n",
        "    .from_tensor_slices(test_paths)\n",
        "    .map(decode_image, num_parallel_calls=AUTO)\n",
        "    .batch(BATCH_SIZE)\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xmirtR2L7TDC"
      },
      "cell_type": "markdown",
      "source": [
        "### Helper functions"
      ]
    },
    {
      "metadata": {
        "id": "uiiCB9SdpnMc",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "#learning rate scheduler\n",
        "def build_lrfn(lr_start=0.00001, lr_max=0.00005,\n",
        "               lr_min=0.00001, lr_rampup_epochs=5,\n",
        "               lr_sustain_epochs=0, lr_exp_decay=.8):\n",
        "    lr_max = lr_max * strategy.num_replicas_in_sync\n",
        "\n",
        "    def lrfn(epoch):\n",
        "        if epoch < lr_rampup_epochs:\n",
        "            lr = (lr_max - lr_start) / lr_rampup_epochs * epoch + lr_start\n",
        "        elif epoch < lr_rampup_epochs + lr_sustain_epochs:\n",
        "            lr = lr_max\n",
        "        else:\n",
        "            lr = (lr_max - lr_min) *\\\n",
        "                 lr_exp_decay**(epoch - lr_rampup_epochs\\\n",
        "                                - lr_sustain_epochs) + lr_min\n",
        "        return lr\n",
        "    return lrfn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dZi9qF4tngbA"
      },
      "cell_type": "markdown",
      "source": [
        "### Define hyperparameters and callbacks"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "a7MnCZzRngbA"
      },
      "cell_type": "code",
      "source": [
        "lrfn = build_lrfn()\n",
        "STEPS_PER_EPOCH = train_labels.shape[0] // BATCH_SIZE\n",
        "lr_schedule = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PRtfn6bJ7qI1"
      },
      "cell_type": "markdown",
      "source": [
        "## DenseNet <a id=\"3.2\"></a>\n",
        "\n",
        "Densely Connected Convolutional Networks (DenseNets), are a popular CNN-based ImageNet used for a variety of applications, inclusing classification, segmentation, localization, etc. Most models before DenseNet relied solely on network depth for representational power. **Instead of drawing representational power from extremely deep or wide architectures, DenseNets exploit the potential of the network through feature reuse.** This was the main motivation behind the DenseNet architecture. Now let us train DenseNet on leaf images and evaluate its performance."
      ]
    },
    {
      "metadata": {
        "id": "7fllHhN9pnMe",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "with strategy.scope():\n",
        "    model = tf.keras.Sequential([DenseNet121(input_shape=(512, 512, 3),\n",
        "                                             weights='imagenet',\n",
        "                                             include_top=False),\n",
        "                                 L.GlobalAveragePooling2D(),\n",
        "                                 L.Dense(train_labels.shape[1],\n",
        "                                         activation='softmax')])\n",
        "\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss = 'categorical_crossentropy',\n",
        "                  metrics=['categorical_accuracy'])\n",
        "    model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "V4yafObR7wIo"
      },
      "cell_type": "markdown",
      "source": [
        "### Train model"
      ]
    },
    {
      "metadata": {
        "_kg_hide-input": false,
        "_kg_hide-output": true,
        "id": "V56dwx6opnMh",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "history = model.fit(train_dataset,\n",
        "                    epochs=EPOCHS,\n",
        "                    callbacks=[lr_schedule],\n",
        "                    steps_per_epoch=STEPS_PER_EPOCH,\n",
        "                    validation_data=valid_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "H32GrB0L7ulh"
      },
      "cell_type": "markdown",
      "source": [
        "### Visualize results"
      ]
    },
    {
      "metadata": {
        "_kg_hide-input": true,
        "trusted": true,
        "id": "Owk6sOzCngbE"
      },
      "cell_type": "code",
      "source": [
        "def display_training_curves(training, validation, yaxis):\n",
        "    if yaxis == \"loss\":\n",
        "        ylabel = \"Loss\"\n",
        "        title = \"Loss vs. Epochs\"\n",
        "    else:\n",
        "        ylabel = \"Accuracy\"\n",
        "        title = \"Accuracy vs. Epochs\"\n",
        "\n",
        "    fig = go.Figure()\n",
        "\n",
        "    fig.add_trace(\n",
        "        go.Scatter(x=np.arange(1, EPOCHS+1), mode='lines+markers', y=training, marker=dict(color=\"dodgerblue\"),\n",
        "               name=\"Train\"))\n",
        "\n",
        "    fig.add_trace(\n",
        "        go.Scatter(x=np.arange(1, EPOCHS+1), mode='lines+markers', y=validation, marker=dict(color=\"darkorange\"),\n",
        "               name=\"Val\"))\n",
        "\n",
        "    fig.update_layout(title_text=title, yaxis_title=ylabel, xaxis_title=\"Epochs\", template=\"plotly_white\")\n",
        "    fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "U2Rmet4p-ot3"
      },
      "cell_type": "markdown",
      "source": [
        "### Scatter plots"
      ]
    },
    {
      "metadata": {
        "_kg_hide-input": true,
        "id": "dKUl8NckpnMn",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "display_training_curves(\n",
        "    history.history['categorical_accuracy'],\n",
        "    history.history['val_categorical_accuracy'],\n",
        "    'accuracy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jEF12Tq0ngbF"
      },
      "cell_type": "markdown",
      "source": [
        "From the above plots, we can see that the losses decrease and accuracies increase quite consistently. The training metrics settle down very fast (after 1 or 2 epochs), whereas the validation metrics much greater volatility and start to settle down only after 7-8 epochs. This is expected because validation data is unseen and more diffcult to make predictions on than training data."
      ]
    },
    {
      "metadata": {
        "id": "Vu8SzJGQngbF"
      },
      "cell_type": "markdown",
      "source": [
        "### Animation (click ▶️)"
      ]
    },
    {
      "metadata": {
        "_kg_hide-input": true,
        "trusted": true,
        "id": "qnua45GlngbG"
      },
      "cell_type": "code",
      "source": [
        "acc_df = pd.DataFrame(np.transpose([[*np.arange(1, EPOCHS+1).tolist()*3], [\"Train\"]*EPOCHS + [\"Val\"]*EPOCHS + [\"Benchmark\"]*EPOCHS,\n",
        "                                     history.history['categorical_accuracy'] + history.history['val_categorical_accuracy'] + [1.0]*EPOCHS]))\n",
        "acc_df.columns = [\"Epochs\", \"Stage\", \"Accuracy\"]\n",
        "fig = px.bar(acc_df, x=\"Accuracy\", y=\"Stage\", animation_frame=\"Epochs\", title=\"Accuracy vs. Epochs\", color='Stage',\n",
        "       color_discrete_map={\"Train\":\"dodgerblue\", \"Val\":\"darkorange\", \"Benchmark\":\"seagreen\"}, orientation=\"h\")\n",
        "\n",
        "fig.update_layout(\n",
        "    xaxis = dict(\n",
        "        autorange=False,\n",
        "        range=[0, 1]\n",
        "    )\n",
        ")\n",
        "\n",
        "fig.update_layout(template=\"plotly_white\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CyX0-nrJngbG"
      },
      "cell_type": "markdown",
      "source": [
        "From the animations above, we can see the volatility in validation metrics a lot more clearly. The validation metrics oscillate in an erratic fashion until it reaches the 7th epoch and starts to generalize properly."
      ]
    },
    {
      "metadata": {
        "id": "gl6JY-q7ngbG"
      },
      "cell_type": "markdown",
      "source": [
        "### Sample predictions\n",
        "\n",
        "Now, I will visualize some sample predictions made by the DenseNet model. The <font color=\"red\">red</font> bars represent the model's prediction (maximum probability), the <font color=\"green\">green</font> represent the ground truth (label), and the rest of the bars are <font color=\"blue\">blue</font>. When the model predicts correctly, the prediction bar is <font color=\"green\">green</font>."
      ]
    },
    {
      "metadata": {
        "_kg_hide-input": true,
        "trusted": true,
        "id": "4mQlx3S3ngbG"
      },
      "cell_type": "code",
      "source": [
        "def process(img):\n",
        "    return cv2.resize(img/255.0, (512, 512)).reshape(-1, 512, 512, 3)\n",
        "def predict(img):\n",
        "    return model.layers[2](model.layers[1](model.layers[0](process(img)))).numpy()[0]\n",
        "\n",
        "fig = make_subplots(rows=4, cols=2)\n",
        "preds = predict(train_images[2])\n",
        "\n",
        "colors = {\"Healthy\":px.colors.qualitative.Plotly[0], \"Scab\":px.colors.qualitative.Plotly[0], \"Rust\":px.colors.qualitative.Plotly[0], \"Multiple diseases\":px.colors.qualitative.Plotly[0]}\n",
        "if list.index(preds.tolist(), max(preds)) == 0:\n",
        "    pred = \"Healthy\"\n",
        "if list.index(preds.tolist(), max(preds)) == 1:\n",
        "    pred = \"Scab\"\n",
        "if list.index(preds.tolist(), max(preds)) == 2:\n",
        "    pred = \"Rust\"\n",
        "if list.index(preds.tolist(), max(preds)) == 3:\n",
        "    pred = \"Multiple diseases\"\n",
        "\n",
        "colors[pred] = px.colors.qualitative.Plotly[1]\n",
        "colors[\"Healthy\"] = \"seagreen\"\n",
        "colors = [colors[val] for val in colors.keys()]\n",
        "fig.add_trace(go.Image(z=cv2.resize(train_images[2], (205, 136))), row=1, col=1)\n",
        "fig.add_trace(go.Bar(x=[\"Healthy\", \"Multiple diseases\", \"Rust\", \"Scab\"], y=preds, marker=dict(color=colors)), row=1, col=2)\n",
        "fig.update_layout(height=1200, width=800, title_text=\"DenseNet Predictions\", showlegend=False)\n",
        "\n",
        "preds = predict(train_images[0])\n",
        "colors = {\"Healthy\":px.colors.qualitative.Plotly[0], \"Scab\":px.colors.qualitative.Plotly[0], \"Rust\":px.colors.qualitative.Plotly[0], \"Multiple diseases\":px.colors.qualitative.Plotly[0]}\n",
        "if list.index(preds.tolist(), max(preds)) == 0:\n",
        "    pred = \"Healthy\"\n",
        "if list.index(preds.tolist(), max(preds)) == 1:\n",
        "    pred = \"Multiple diseases\"\n",
        "if list.index(preds.tolist(), max(preds)) == 2:\n",
        "    pred = \"Rust\"\n",
        "if list.index(preds.tolist(), max(preds)) == 3:\n",
        "    pred = \"Scab\"\n",
        "\n",
        "colors[pred] = px.colors.qualitative.Plotly[1]\n",
        "colors[\"Multiple diseases\"] = \"seagreen\"\n",
        "colors = [colors[val] for val in colors.keys()]\n",
        "fig.add_trace(go.Image(z=cv2.resize(train_images[0], (205, 136))), row=2, col=1)\n",
        "fig.add_trace(go.Bar(x=[\"Healthy\", \"Multiple diseases\", \"Rust\", \"Scab\"], y=preds, marker=dict(color=colors)), row=2, col=2)\n",
        "\n",
        "preds = predict(train_images[3])\n",
        "colors = {\"Healthy\":px.colors.qualitative.Plotly[0], \"Scab\":px.colors.qualitative.Plotly[0], \"Rust\":px.colors.qualitative.Plotly[0], \"Multiple diseases\":px.colors.qualitative.Plotly[0]}\n",
        "if list.index(preds.tolist(), max(preds)) == 0:\n",
        "    pred = \"Healthy\"\n",
        "if list.index(preds.tolist(), max(preds)) == 1:\n",
        "    pred = \"Multiple diseases\"\n",
        "if list.index(preds.tolist(), max(preds)) == 2:\n",
        "    pred = \"Rust\"\n",
        "if list.index(preds.tolist(), max(preds)) == 3:\n",
        "    pred = \"Scab\"\n",
        "\n",
        "colors[pred] = px.colors.qualitative.Plotly[1]\n",
        "colors[\"Rust\"] = \"seagreen\"\n",
        "colors = [colors[val] for val in colors.keys()]\n",
        "fig.add_trace(go.Image(z=cv2.resize(train_images[3], (205, 136))), row=3, col=1)\n",
        "fig.add_trace(go.Bar(x=[\"Healthy\", \"Multiple diseases\", \"Rust\", \"Scab\"], y=preds, marker=dict(color=colors)), row=3, col=2)\n",
        "\n",
        "preds = predict(train_images[1])\n",
        "colors = {\"Healthy\":px.colors.qualitative.Plotly[0], \"Scab\":px.colors.qualitative.Plotly[0], \"Rust\":px.colors.qualitative.Plotly[0], \"Multiple diseases\":px.colors.qualitative.Plotly[0]}\n",
        "if list.index(preds.tolist(), max(preds)) == 0:\n",
        "    pred = \"Healthy\"\n",
        "if list.index(preds.tolist(), max(preds)) == 1:\n",
        "    pred = \"Multiple diseases\"\n",
        "if list.index(preds.tolist(), max(preds)) == 2:\n",
        "    pred = \"Rust\"\n",
        "if list.index(preds.tolist(), max(preds)) == 3:\n",
        "    pred = \"Scab\"\n",
        "\n",
        "colors[pred] = px.colors.qualitative.Plotly[1]\n",
        "colors[\"Scab\"] = \"seagreen\"\n",
        "colors = [colors[val] for val in colors.keys()]\n",
        "fig.add_trace(go.Image(z=cv2.resize(train_images[1], (205, 136))), row=4, col=1)\n",
        "fig.add_trace(go.Bar(x=[\"Healthy\", \"Multiple diseases\", \"Rust\", \"Scab\"], y=preds, marker=dict(color=colors)), row=4, col=2)\n",
        "\n",
        "fig.update_layout(template=\"plotly_white\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CCL9-QINngbH"
      },
      "cell_type": "markdown",
      "source": [
        "We can see that DenseNet predicts leaf diseases with great accuracy. No red or blue bars are seen. The probabilities are very polarized (one very high and the rest very low), indicating that the model is making these predictions with great confidence."
      ]
    },
    {
      "metadata": {
        "id": "AgQWDNKtngbH"
      },
      "cell_type": "markdown",
      "source": [
        "### Generate submission"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "YOnriilAngbH"
      },
      "cell_type": "code",
      "source": [
        "probs_dnn = model.predict(test_dataset, verbose=1)\n",
        "sub.loc[:, 'healthy':] = probs_dnn\n",
        "sub.to_csv('submission_dnn.csv', index=False)\n",
        "sub.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# ... (Your existing code for model training and prediction) ...\n",
        "\n",
        "# Assuming 'valid_dataset' contains your validation data (images and labels)\n",
        "# Get the true labels from the validation dataset\n",
        "y_true = np.concatenate([y for x, y in valid_dataset], axis=0)\n",
        "\n",
        "# Get predictions from the model on the validation dataset\n",
        "y_pred = model.predict(valid_dataset)\n",
        "y_pred = np.argmax(y_pred, axis=1)  # Convert predictions to class labels\n",
        "\n",
        "# Convert one-hot encoded y_true to class labels if necessary\n",
        "if y_true.ndim > 1 and y_true.shape[1] > 1:  # Check if y_true is one-hot encoded\n",
        "    y_true = np.argmax(y_true, axis=1)\n",
        "\n",
        "# Print the classification report\n",
        "target_names = ['healthy', 'multiple_diseases', 'rust', 'scab']  # Adjust class names if needed\n",
        "print(classification_report(y_true, y_pred, target_names=target_names))"
      ],
      "metadata": {
        "id": "ZdfH_DcdKv8T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# ... (Your existing code for model training and prediction) ...\n",
        "\n",
        "# Assuming 'valid_dataset' contains your validation data (images and labels)\n",
        "# Get the true labels from the validation dataset\n",
        "y_true = np.concatenate([y for x, y in valid_dataset], axis=0)\n",
        "\n",
        "# Get predictions from the model on the validation dataset\n",
        "y_pred = model.predict(valid_dataset)\n",
        "y_pred = np.argmax(y_pred, axis=1)  # Convert predictions to class labels\n",
        "\n",
        "# Convert one-hot encoded y_true to class labels if necessary\n",
        "if y_true.ndim > 1 and y_true.shape[1] > 1:  # Check if y_true is one-hot encoded\n",
        "    y_true = np.argmax(y_true, axis=1)\n",
        "\n",
        "# Calculate and print the accuracy\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "id": "0YMkqmzlLa9z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score\n",
        "\n",
        "# ... (Your existing code for model training and prediction) ...\n",
        "\n",
        "# Assuming 'valid_dataset' contains your validation data (images and labels)\n",
        "# Get the true labels from the validation dataset\n",
        "y_true = np.concatenate([y for x, y in valid_dataset], axis=0)\n",
        "\n",
        "# Get predictions from the model on the validation dataset\n",
        "y_pred = model.predict(valid_dataset)\n",
        "y_pred = np.argmax(y_pred, axis=1)  # Convert predictions to class labels\n",
        "\n",
        "# Convert one-hot encoded y_true to class labels if necessary\n",
        "if y_true.ndim > 1 and y_true.shape[1] > 1:  # Check if y_true is one-hot encoded\n",
        "    y_true = np.argmax(y_true, axis=1)\n",
        "\n",
        "# Calculate and print precision\n",
        "precision = precision_score(y_true, y_pred, average='weighted')  # or average='macro'/'micro'\n",
        "print(f\"Precision: {precision}\")"
      ],
      "metadata": {
        "id": "n-a5T_FhLmI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import recall_score\n",
        "\n",
        "# ... (Your existing code for model training and prediction) ...\n",
        "\n",
        "# Assuming 'valid_dataset' contains your validation data (images and labels)\n",
        "# Get the true labels from the validation dataset\n",
        "y_true = np.concatenate([y for x, y in valid_dataset], axis=0)\n",
        "\n",
        "# Get predictions from the model on the validation dataset\n",
        "y_pred = model.predict(valid_dataset)\n",
        "y_pred = np.argmax(y_pred, axis=1)  # Convert predictions to class labels\n",
        "\n",
        "# Convert one-hot encoded y_true to class labels if necessary\n",
        "if y_true.ndim > 1 and y_true.shape[1] > 1:  # Check if y_true is one-hot encoded\n",
        "    y_true = np.argmax(y_true, axis=1)\n",
        "\n",
        "# Calculate and print recall\n",
        "recall = recall_score(y_true, y_pred, average='weighted')  # or average='macro'/'micro'\n",
        "print(f\"Recall: {recall}\")"
      ],
      "metadata": {
        "id": "8opwyCU3Lw9D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# ... (Your existing code for model training and prediction) ...\n",
        "\n",
        "# Assuming 'valid_dataset' contains your validation data (images and labels)\n",
        "# Get the true labels from the validation dataset\n",
        "y_true = np.concatenate([y for x, y in valid_dataset], axis=0)\n",
        "\n",
        "# Get predictions from the model on the validation dataset\n",
        "y_pred = model.predict(valid_dataset)\n",
        "y_pred = np.argmax(y_pred, axis=1)  # Convert predictions to class labels\n",
        "\n",
        "# Convert one-hot encoded y_true to class labels if necessary\n",
        "if y_true.ndim > 1 and y_true.shape[1] > 1:  # Check if y_true is one-hot encoded\n",
        "    y_true = np.argmax(y_true, axis=1)\n",
        "\n",
        "# Calculate and print F1-score\n",
        "f1 = f1_score(y_true, y_pred, average='weighted')  # or average='macro'/'micro'\n",
        "print(f\"F1-score: {f1}\")"
      ],
      "metadata": {
        "id": "eeVwjtSqLzoN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ... (Your existing code for model training and prediction) ...\n",
        "\n",
        "# Assuming 'valid_dataset' contains your validation data (images and labels)\n",
        "# Get the true labels from the validation dataset\n",
        "y_true = np.concatenate([y for x, y in valid_dataset], axis=0)\n",
        "\n",
        "# Get predictions from the model on the validation dataset\n",
        "y_pred = model.predict(valid_dataset)\n",
        "y_pred = np.argmax(y_pred, axis=1)  # Convert predictions to class labels\n",
        "\n",
        "# Convert one-hot encoded y_true to class labels if necessary\n",
        "if y_true.ndim > 1 and y_true.shape[1] > 1:  # Check if y_true is one-hot encoded\n",
        "    y_true = np.argmax(y_true, axis=1)\n",
        "\n",
        "# Calculate confusion matrix\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "# Plot the confusion matrix using seaborn heatmap\n",
        "target_names = ['healthy', 'multiple_diseases', 'rust', 'scab']  # Replace with your class names\n",
        "plt.figure(figsize=(10, 8))  # Adjust figure size if needed\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=target_names, yticklabels=target_names)\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QnVUJpbwMHuG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# ... (Your existing code for model training and prediction) ...\n",
        "\n",
        "# Assuming 'valid_dataset' contains your validation data (images and labels)\n",
        "# Get the true labels from the validation dataset\n",
        "y_true = np.concatenate([y for x, y in valid_dataset], axis=0)\n",
        "\n",
        "# Get predictions from the model on the validation dataset\n",
        "y_pred = model.predict(valid_dataset)\n",
        "y_pred = np.argmax(y_pred, axis=1)  # Convert predictions to class labels\n",
        "\n",
        "# Convert one-hot encoded y_true to class labels if necessary\n",
        "if y_true.ndim > 1 and y_true.shape[1] > 1:  # Check if y_true is one-hot encoded\n",
        "    y_true = np.argmax(y_true, axis=1)\n",
        "\n",
        "# Calculate confusion matrix\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "# Calculate accuracy from confusion matrix\n",
        "accuracy = np.trace(cm) / np.sum(cm)\n",
        "\n",
        "# Print accuracy\n",
        "print(f\"Accuracy (from confusion matrix): {accuracy}\")\n",
        "\n",
        "# ... (Rest of the code for plotting the confusion matrix remains the same) ..."
      ],
      "metadata": {
        "id": "4eaJbxL9Mbh8"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}